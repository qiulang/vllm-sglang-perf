```
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./sglang-stress-test.py 
Starting SGLang Stress Test
URL: http://localhost:30000/generate
Max Tokens: 256
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 37.80 seconds
        SGLang Stress Test Results - 2025-05-23 20:06:20        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                           ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:30000/generate │
│ Max Tokens                 │ 256                             │
│ Concurrent Requests        │ 10                              │
│ Total Requests             │ 100                             │
│ Successful Requests        │ 100 (100.0%)                    │
│ Failed Requests            │ 0 (0.0%)                        │
│ Total Test Duration        │ 37.80 seconds                   │
│ Min Response Time          │ 2.27 seconds                    │
│ Max Response Time          │ 16.93 seconds                   │
│ Average Response Time      │ 3.78 seconds                    │
│ Median Response Time       │ 2.30 seconds                    │
│ P90 Response Time          │ 16.93 seconds                   │
│ P95 Response Time          │ 16.93 seconds                   │
│ P99 Response Time          │ 16.93 seconds                   │
│ Std Dev Response Time      │ 4.41 seconds                    │
│ Theoretical Max Throughput │ 5.91 requests/second            │
│ Actual Throughput          │ 2.65 requests/second            │
│ Total Generated Tokens     │ 9273                            │
│ Tokens Per Second          │ 245.34                          │
│ Avg Tokens Per Request     │ 92.73                           │
│ Peak Requests In Flight    │ 10                              │
└────────────────────────────┴─────────────────────────────────┘

Sample Response:

/pretend
Okay, so I need to explain how public ke...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi sglang-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./sglang-stress-test.py 
Starting SGLang Stress Test
URL: http://localhost:30000/generate
Max Tokens: 256
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 23.02 seconds
        SGLang Stress Test Results - 2025-05-23 20:10:25        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                           ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:30000/generate │
│ Max Tokens                 │ 256                             │
│ Concurrent Requests        │ 10                              │
│ Total Requests             │ 100                             │
│ Successful Requests        │ 100 (100.0%)                    │
│ Failed Requests            │ 0 (0.0%)                        │
│ Total Test Duration        │ 23.02 seconds                   │
│ Min Response Time          │ 2.27 seconds                    │
│ Max Response Time          │ 2.33 seconds                    │
│ Average Response Time      │ 2.30 seconds                    │
│ Median Response Time       │ 2.30 seconds                    │
│ P90 Response Time          │ 2.33 seconds                    │
│ P95 Response Time          │ 2.33 seconds                    │
│ P99 Response Time          │ 2.33 seconds                    │
│ Std Dev Response Time      │ 0.02 seconds                    │
│ Theoretical Max Throughput │ 42.84 requests/second           │
│ Actual Throughput          │ 4.34 requests/second            │
│ Total Generated Tokens     │ 9953                            │
│ Tokens Per Second          │ 432.45                          │
│ Avg Tokens Per Request     │ 99.53                           │
│ Peak Requests In Flight    │ 10                              │
└────────────────────────────┴─────────────────────────────────┘

Sample Response:
 Also, provide a real-world example that illustrat...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./sglang-stress-test.py 
Starting SGLang Stress Test
URL: http://localhost:30000/generate
Max Tokens: 256
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 22.82 seconds
        SGLang Stress Test Results - 2025-05-23 20:10:56        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                           ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:30000/generate │
│ Max Tokens                 │ 256                             │
│ Concurrent Requests        │ 10                              │
│ Total Requests             │ 100                             │
│ Successful Requests        │ 100 (100.0%)                    │
│ Failed Requests            │ 0 (0.0%)                        │
│ Total Test Duration        │ 22.82 seconds                   │
│ Min Response Time          │ 2.25 seconds                    │
│ Max Response Time          │ 2.31 seconds                    │
│ Average Response Time      │ 2.28 seconds                    │
│ Median Response Time       │ 2.28 seconds                    │
│ P90 Response Time          │ 2.31 seconds                    │
│ P95 Response Time          │ 2.31 seconds                    │
│ P99 Response Time          │ 2.31 seconds                    │
│ Std Dev Response Time      │ 0.02 seconds                    │
│ Theoretical Max Throughput │ 43.29 requests/second           │
│ Actual Throughput          │ 4.38 requests/second            │
│ Total Generated Tokens     │ 10282                           │
│ Tokens Per Second          │ 450.53                          │
│ Avg Tokens Per Request     │ 102.82                          │
│ Peak Requests In Flight    │ 10                              │
└────────────────────────────┴─────────────────────────────────┘

Sample Response:
 Give an example of its use in real-world applicat...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ls
prompt.txt  requirements.txt  sglang-stress-test.py  val3.py  vllm-stress-test.py  号百反诈测试集.xlsx
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi vllm-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# chmod +x vllm-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./vllm-stress-test.py 
Starting vLLM Stress Test
URL: http://localhost:8000/v1/completions
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 26.49 seconds
           vLLM Stress Test Results - 2025-05-23 20:17:38            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:8000/v1/completions │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B         │
│ Max Tokens                 │ 256                                  │
│ Temperature                │ 0.7                                  │
│ Concurrent Requests        │ 10                                   │
│ Total Requests             │ 100                                  │
│ Successful Requests        │ 100 (100.0%)                         │
│ Failed Requests            │ 0 (0.0%)                             │
│ Total Test Duration        │ 26.49 seconds                        │
│ Min Response Time          │ 2.62 seconds                         │
│ Max Response Time          │ 2.70 seconds                         │
│ Average Response Time      │ 2.65 seconds                         │
│ Median Response Time       │ 2.64 seconds                         │
│ P90 Response Time          │ 2.68 seconds                         │
│ P95 Response Time          │ 2.70 seconds                         │
│ P99 Response Time          │ 2.70 seconds                         │
│ Std Dev Response Time      │ 0.02 seconds                         │
│ Theoretical Max Throughput │ 37.08 requests/second                │
│ Actual Throughput          │ 3.78 requests/second                 │
│ Total Generated Tokens     │ 20464                                │
│ Tokens Per Second          │ 772.54                               │
│ Avg Tokens Per Request     │ 204.64                               │
│ Peak Requests In Flight    │ 10                                   │
└────────────────────────────┴──────────────────────────────────────┘
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./vllm-stress-test.py 
Starting vLLM Stress Test
URL: http://localhost:8000/v1/completions
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 26.39 seconds
           vLLM Stress Test Results - 2025-05-23 20:19:07            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:8000/v1/completions │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B         │
│ Max Tokens                 │ 256                                  │
│ Temperature                │ 0.7                                  │
│ Concurrent Requests        │ 10                                   │
│ Total Requests             │ 100                                  │
│ Successful Requests        │ 100 (100.0%)                         │
│ Failed Requests            │ 0 (0.0%)                             │
│ Total Test Duration        │ 26.39 seconds                        │
│ Min Response Time          │ 2.62 seconds                         │
│ Max Response Time          │ 2.65 seconds                         │
│ Average Response Time      │ 2.64 seconds                         │
│ Median Response Time       │ 2.64 seconds                         │
│ P90 Response Time          │ 2.64 seconds                         │
│ P95 Response Time          │ 2.65 seconds                         │
│ P99 Response Time          │ 2.65 seconds                         │
│ Std Dev Response Time      │ 0.01 seconds                         │
│ Theoretical Max Throughput │ 37.79 requests/second                │
│ Actual Throughput          │ 3.79 requests/second                 │
│ Total Generated Tokens     │ 20418                                │
│ Tokens Per Second          │ 773.71                               │
│ Avg Tokens Per Request     │ 204.18                               │
│ Peak Requests In Flight    │ 10                                   │
└────────────────────────────┴──────────────────────────────────────┘
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi vllm-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./!$
./vllm-stress-test.py
Starting vLLM Stress Test
URL: http://localhost:8000/v1/completions
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 26.53 seconds
           vLLM Stress Test Results - 2025-05-23 20:21:45            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:8000/v1/completions │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B         │
│ Max Tokens                 │ 256                                  │
│ Temperature                │ 0.7                                  │
│ Concurrent Requests        │ 10                                   │
│ Total Requests             │ 100                                  │
│ Successful Requests        │ 100 (100.0%)                         │
│ Failed Requests            │ 0 (0.0%)                             │
│ Total Test Duration        │ 26.53 seconds                        │
│ Min Response Time          │ 2.63 seconds                         │
│ Max Response Time          │ 2.67 seconds                         │
│ Average Response Time      │ 2.65 seconds                         │
│ Median Response Time       │ 2.65 seconds                         │
│ P90 Response Time          │ 2.66 seconds                         │
│ P95 Response Time          │ 2.67 seconds                         │
│ P99 Response Time          │ 2.67 seconds                         │
│ Std Dev Response Time      │ 0.01 seconds                         │
│ Theoretical Max Throughput │ 37.43 requests/second                │
│ Actual Throughput          │ 3.77 requests/second                 │
│ Total Generated Tokens     │ 20521                                │
│ Tokens Per Second          │ 773.50                               │
│ Avg Tokens Per Request     │ 205.21                               │
│ Peak Requests In Flight    │ 10                                   │
└────────────────────────────┴──────────────────────────────────────┘
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi vllm-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./vllm-stress-test.py --concurrent 1 --total 1
Starting vLLM Stress Test
URL: http://localhost:8000/v1/completions
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 1
Total requests: 1
 Also, explain why it is important in the context of distributed systems.

The CAP theorem, also known as the "Consistency, Availability, and Partition tolerance" theorem, is a fundamental principle in distributed 
computing. It states that a distributed system can only guarantee two out of three of the following three properties at any given time:

1. **Consistency (C):** All nodes in the system must agree on the current state of the data. In other words, every read operation returns 
the latest written value, and all writes are immediately visible to all clients.

2. **Availability (A):** The system must be able to handle requests at any time, meaning that the system is always responsive and can 
process read or write operations. If a request is made, it should be processed, even if the system is under load or a node is down.

3. **Partition Tolerance (P):** The system must continue to function properly even if there is a network partition, which is a situation 
where communication between nodes is disrupted. This is a critical aspect of distributed systems because network partitions are inevitable.

The CAP theorem tells us that in a distributed system, we cannot have all three properties simultaneously. The choice between Consistency 
and Availability depends on the system's requirements and constraints.
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 2.13 seconds
           vLLM Stress Test Results - 2025-05-23 20:24:12            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:8000/v1/completions │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B         │
│ Max Tokens                 │ 256                                  │
│ Temperature                │ 0.7                                  │
│ Concurrent Requests        │ 1                                    │
│ Total Requests             │ 1                                    │
│ Successful Requests        │ 1 (100.0%)                           │
│ Failed Requests            │ 0 (0.0%)                             │
│ Total Test Duration        │ 2.13 seconds                         │
│ Min Response Time          │ 2.12 seconds                         │
│ Max Response Time          │ 2.12 seconds                         │
│ Average Response Time      │ 2.12 seconds                         │
│ Median Response Time       │ 2.12 seconds                         │
│ P90 Response Time          │ 2.12 seconds                         │
│ P95 Response Time          │ 2.12 seconds                         │
│ P99 Response Time          │ 2.12 seconds                         │
│ Theoretical Max Throughput │ 0.47 requests/second                 │
│ Actual Throughput          │ 0.47 requests/second                 │
│ Total Generated Tokens     │ 211                                  │
│ Tokens Per Second          │ 99.25                                │
│ Avg Tokens Per Request     │ 211.00                               │
│ Peak Requests In Flight    │ 1                                    │
└────────────────────────────┴──────────────────────────────────────┘
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi vllm-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./vllm-stress-test.py --concurrent 1 --total 1
Starting vLLM Stress Test
URL: http://localhost:8000/v1/completions
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 1
Total requests: 1
<ClientResponse(http://localhost:8000/v1/completions) [200 OK]>
<CIMultiDictProxy('Date': 'Fri, 23 May 2025 12:25:42 GMT', 'Server': 'uvicorn', 'Content-Length': '1739', 'Content-Type': 
'application/json')>

Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 2.12 seconds
           vLLM Stress Test Results - 2025-05-23 20:25:45            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:8000/v1/completions │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B         │
│ Max Tokens                 │ 256                                  │
│ Temperature                │ 0.7                                  │
│ Concurrent Requests        │ 1                                    │
│ Total Requests             │ 1                                    │
│ Successful Requests        │ 1 (100.0%)                           │
│ Failed Requests            │ 0 (0.0%)                             │
│ Total Test Duration        │ 2.12 seconds                         │
│ Min Response Time          │ 2.12 seconds                         │
│ Max Response Time          │ 2.12 seconds                         │
│ Average Response Time      │ 2.12 seconds                         │
│ Median Response Time       │ 2.12 seconds                         │
│ P90 Response Time          │ 2.12 seconds                         │
│ P95 Response Time          │ 2.12 seconds                         │
│ P99 Response Time          │ 2.12 seconds                         │
│ Theoretical Max Throughput │ 0.47 requests/second                 │
│ Actual Throughput          │ 0.47 requests/second                 │
│ Total Generated Tokens     │ 213                                  │
│ Tokens Per Second          │ 100.27                               │
│ Avg Tokens Per Request     │ 213.00                               │
│ Peak Requests In Flight    │ 1                                    │
└────────────────────────────┴──────────────────────────────────────┘
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi vllm-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./vllm-stress-test.py --concurrent 1 --total 1
Starting vLLM Stress Test
URL: http://localhost:8000/v1/completions
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 1
Total requests: 1
{'id': 'cmpl-59e7ee4b32624dc59fd57c24e5179f6c', 'object': 'text_completion', 'created': 1748003183, 'model': '/home/vllm/llm/Qwen/Qwen3-4B',
'choices': [{'index': 0, 'text': " What are some common algorithms for each type? What are the advantages and disadvantages of each type? 
How do they compare in terms of performance and data requirements? What are some practical applications of each type?\n\nSupervised and 
unsupervised machine learning are two fundamental types of machine learning, each with distinct approaches to learning from data. Here's a 
structured comparison:\n\n---\n\n### **1. Supervised Machine Learning**\n- **Definition**: Uses labeled data (input-output pairs) to train 
models. The goal is to learn a mapping from inputs to outputs.\n- **Common Algorithms**:\n  - **Classification**: Logistic Regression, 
Decision Trees, Support Vector Machines (SVM), Random Forests, Neural Networks.\n  - **Regression**: Linear Regression, Polynomial 
Regression, Ridge Regression, Lasso Regression.\n- **Advantages**:\n  - High accuracy when labeled data is available.\n  - Directly 
optimizes for prediction tasks.\n  - Interpretable models (e.g., decision trees).\n- **Disadvantages**:\n  - Requires labeled data, which is
costly and time-consuming to create.\n  - Sensitive to noise and outliers in labels.\n  - May overfit if not properly regularized.\n- 
**Performance & Data Requirements**:\n  - High performance on well-l", 'logprobs': None, 'finish_reason': 'length', 'stop_reason': None, 
'prompt_logprobs': None}], 'usage': {'prompt_tokens': 13, 'total_tokens': 269, 'completion_tokens': 256, 'prompt_tokens_details': None}}
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 2.13 seconds
           vLLM Stress Test Results - 2025-05-23 20:26:25            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:8000/v1/completions │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B         │
│ Max Tokens                 │ 256                                  │
│ Temperature                │ 0.7                                  │
│ Concurrent Requests        │ 1                                    │
│ Total Requests             │ 1                                    │
│ Successful Requests        │ 1 (100.0%)                           │
│ Failed Requests            │ 0 (0.0%)                             │
│ Total Test Duration        │ 2.13 seconds                         │
│ Min Response Time          │ 2.12 seconds                         │
│ Max Response Time          │ 2.12 seconds                         │
│ Average Response Time      │ 2.12 seconds                         │
│ Median Response Time       │ 2.12 seconds                         │
│ P90 Response Time          │ 2.12 seconds                         │
│ P95 Response Time          │ 2.12 seconds                         │
│ P99 Response Time          │ 2.12 seconds                         │
│ Theoretical Max Throughput │ 0.47 requests/second                 │
│ Actual Throughput          │ 0.47 requests/second                 │
│ Total Generated Tokens     │ 174                                  │
│ Tokens Per Second          │ 81.84                                │
│ Avg Tokens Per Request     │ 174.00                               │
│ Peak Requests In Flight    │ 1                                    │
└────────────────────────────┴──────────────────────────────────────┘
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi vllm-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./vllm-stress-test.py --concurrent 1 --total 1
Starting vLLM Stress Test
URL: http://localhost:8000/v1/completions
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 1
Total requests: 1
{'id': 'cmpl-7ff8ccee141d4ce9a231489135ec9aff', 'object': 'text_completion', 'created': 1748003327, 'model': '/home/vllm/llm/Qwen/Qwen3-4B',
'choices': [{'index': 0, 'text': ' A good API (Application Programming Interface) is a set of rules and protocols that allows different 
software applications to communicate and interact with each other efficiently. It defines how software components should interact, 
specifying the methods, data formats, and authentication mechanisms that must be used. A good API is intuitive, well-documented, and 
consistent, making it easy for developers to understand and use. It should be reliable, secure, and scalable, ensuring that it can handle 
increasing loads and protect user data. Additionally, a good API should be flexible, allowing for future enhancements and integration with 
various technologies. By providing clear and concise documentation, a good API enables developers to build upon it effectively, fostering 
innovation and collaboration across different systems and platforms.\nOkay, I need to explain what makes a good API in one paragraph. Let me
start by recalling the key points. A good API should allow different software to communicate effectively. It needs to have clear rules and 
protocols. Maybe mention things like methods, data formats, and authentication. Also, it should be well-documented and consistent. 
Reliability and security are important too. Scalability is another factor, so it can handle more users or data. Flexibility for future 
changes and integration with various technologies. Oh, and clear documentation helps developers use it properly.', 'logprobs': None, 
'finish_reason': 'length', 'stop_reason': None, 'prompt_logprobs': None}], 'usage': {'prompt_tokens': 11, 'total_tokens': 267, 
'completion_tokens': 256, 'prompt_tokens_details': None}}
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 2.12 seconds
           vLLM Stress Test Results - 2025-05-23 20:28:49            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:8000/v1/completions │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B         │
│ Max Tokens                 │ 256                                  │
│ Temperature                │ 0.7                                  │
│ Concurrent Requests        │ 1                                    │
│ Total Requests             │ 1                                    │
│ Successful Requests        │ 1 (100.0%)                           │
│ Failed Requests            │ 0 (0.0%)                             │
│ Total Test Duration        │ 2.12 seconds                         │
│ Min Response Time          │ 2.12 seconds                         │
│ Max Response Time          │ 2.12 seconds                         │
│ Average Response Time      │ 2.12 seconds                         │
│ Median Response Time       │ 2.12 seconds                         │
│ P90 Response Time          │ 2.12 seconds                         │
│ P95 Response Time          │ 2.12 seconds                         │
│ P99 Response Time          │ 2.12 seconds                         │
│ Theoretical Max Throughput │ 0.47 requests/second                 │
│ Actual Throughput          │ 0.47 requests/second                 │
│ Total Generated Tokens     │ 212                                  │
│ Tokens Per Second          │ 99.79                                │
│ Avg Tokens Per Request     │ 212.00                               │
│ Peak Requests In Flight    │ 1                                    │
└────────────────────────────┴──────────────────────────────────────┘
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./vllm-stress-test.py --concurrent 1 --total 2
Starting vLLM Stress Test
URL: http://localhost:8000/v1/completions
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 1
Total requests: 2
{'id': 'cmpl-82c1bd17cb5247adb18d4b0cc9accd94', 'object': 'text_completion', 'created': 1748003379, 'model': '/home/vllm/llm/Qwen/Qwen3-4B',
'choices': [{'index': 0, 'text': " How can I implement them?\n\nBest practices for securing a web application include:\n\n1. **Input 
Validation**: Ensure that all user input is validated to prevent injection attacks (e.g., SQL injection, XSS).\n2. **Authentication and 
Authorization**: Implement strong authentication mechanisms and ensure that users are only given access to what they need.\n3. **Secure 
Communication**: Use HTTPS to encrypt data in transit.\n4. **Regular Updates and Patches**: Keep all dependencies and the application itself
up to date to prevent vulnerabilities.\n5. **Error Handling**: Avoid exposing sensitive information in error messages.\n6. **Secure Session 
Management**: Use secure cookies and manage sessions properly to prevent session hijacking.\n7. *CORS (Cross-Origin Resource Sharing)*: 
Configure CORS policies to prevent unauthorized cross-origin requests.\n8. *CSP (Content Security Policy)*: Implement a Content Security 
Policy to mitigate XSS attacks.\n9. *Rate Limiting and DDoS Protection*: Limit the number of requests from a single user to prevent brute 
force attacks.\n10. *Logging and Monitoring*: Keep logs of all activities and monitor for suspicious behavior.\n\nNow, I'll explain how to 
implement each of these in a web application.\n\n---\n\n**1. Input Validation**\n\n- Use libraries like `owasp-java", 'logprobs': None, 
'finish_reason': 'length', 'stop_reason': None, 'prompt_logprobs': None}], 'usage': {'prompt_tokens': 11, 'total_tokens': 267, 
'completion_tokens': 256, 'prompt_tokens_details': None}}
{'id': 'cmpl-f34d9a8470174575a07fdcde01c30ea3', 'object': 'text_completion', 'created': 1748003381, 'model': '/home/vllm/llm/Qwen/Qwen3-4B',
'choices': [{'index': 0, 'text': " Also, explain how the CAP theorem relates to the real world. Additionally, explain the differences 
between the CAP theorem and the BASE theorem. Make sure to keep the answer simple, not too technical, and avoid any jargon. Use simple 
language and analogies.\nImagine you are a systems designer, and you are trying to build a system that handles a lot of data, like a website
that stores user information. You have three main things you need to ensure: Consistency, Availability, and Partition Tolerance. But you 
can't have all three at the same time. So, you have to choose which one to prioritize. \n\nIf you prioritize Consistency, the system must 
ensure that all data is up to date and the same across all parts. But if there's a network problem (Partition Tolerance), you might not be 
able to keep the system available, so you might have to wait for the network to be fixed. If you prioritize Availability, the system should 
always respond to users, even if the data isn't perfectly consistent. But if there's a network problem, you might end up with some data 
being outdated. If you prioritize Partition Tolerance, you make sure the system can handle network issues, but you might have to allow for 
some inconsistency in the data.", 'logprobs': None, 'finish_reason': 'length', 'stop_reason': None, 'prompt_logprobs': None}], 'usage': 
{'prompt_tokens': 9, 'total_tokens': 265, 'completion_tokens': 256, 'prompt_tokens_details': None}}
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 4.24 seconds
           vLLM Stress Test Results - 2025-05-23 20:29:43            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:8000/v1/completions │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B         │
│ Max Tokens                 │ 256                                  │
│ Temperature                │ 0.7                                  │
│ Concurrent Requests        │ 1                                    │
│ Total Requests             │ 2                                    │
│ Successful Requests        │ 2 (100.0%)                           │
│ Failed Requests            │ 0 (0.0%)                             │
│ Total Test Duration        │ 4.24 seconds                         │
│ Min Response Time          │ 2.12 seconds                         │
│ Max Response Time          │ 2.12 seconds                         │
│ Average Response Time      │ 2.12 seconds                         │
│ Median Response Time       │ 2.12 seconds                         │
│ P90 Response Time          │ 2.12 seconds                         │
│ P95 Response Time          │ 2.12 seconds                         │
│ P99 Response Time          │ 2.12 seconds                         │
│ Std Dev Response Time      │ 0.01 seconds                         │
│ Theoretical Max Throughput │ 0.94 requests/second                 │
│ Actual Throughput          │ 0.47 requests/second                 │
│ Total Generated Tokens     │ 399                                  │
│ Tokens Per Second          │ 94.02                                │
│ Avg Tokens Per Request     │ 199.50                               │
│ Peak Requests In Flight    │ 1                                    │
└────────────────────────────┴──────────────────────────────────────┘
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi vllm-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./vllm-stress-test.py
Starting vLLM Stress Test
URL: http://localhost:8000/v1/completions
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 26.54 seconds
           vLLM Stress Test Results - 2025-05-23 20:32:01            
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                                ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:8000/v1/completions │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B         │
│ Max Tokens                 │ 256                                  │
│ Temperature                │ 0.7                                  │
│ Concurrent Requests        │ 10                                   │
│ Total Requests             │ 100                                  │
│ Successful Requests        │ 100 (100.0%)                         │
│ Failed Requests            │ 0 (0.0%)                             │
│ Total Test Duration        │ 26.54 seconds                        │
│ Min Response Time          │ 2.63 seconds                         │
│ Max Response Time          │ 2.66 seconds                         │
│ Average Response Time      │ 2.65 seconds                         │
│ Median Response Time       │ 2.65 seconds                         │
│ P90 Response Time          │ 2.66 seconds                         │
│ P95 Response Time          │ 2.66 seconds                         │
│ P99 Response Time          │ 2.66 seconds                         │
│ Std Dev Response Time      │ 0.01 seconds                         │
│ Theoretical Max Throughput │ 37.55 requests/second                │
│ Actual Throughput          │ 3.77 requests/second                 │
│ Total Generated Tokens     │ 20665                                │
│ Tokens Per Second          │ 778.68                               │
│ Avg Tokens Per Request     │ 206.65                               │
│ Peak Requests In Flight    │ 10                                   │
└────────────────────────────┴──────────────────────────────────────┘
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi vllm-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ls
prompt.txt  requirements.txt  sglang-stress-test.py  val3.py  vllm-stress-test.py  号百反诈测试集.xlsx
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi sglang-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# !$
sglang-stress-test.py
sglang-stress-test.py: command not found
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./!$
./sglang-stress-test.py
Starting SGLang Stress Test
URL: http://localhost:30000/generate
Max Tokens: 256
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 22.88 seconds
        SGLang Stress Test Results - 2025-05-23 20:34:02        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                           ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:30000/generate │
│ Max Tokens                 │ 256                             │
│ Concurrent Requests        │ 10                              │
│ Total Requests             │ 100                             │
│ Successful Requests        │ 100 (100.0%)                    │
│ Failed Requests            │ 0 (0.0%)                        │
│ Total Test Duration        │ 22.88 seconds                   │
│ Min Response Time          │ 2.26 seconds                    │
│ Max Response Time          │ 2.39 seconds                    │
│ Average Response Time      │ 2.29 seconds                    │
│ Median Response Time       │ 2.27 seconds                    │
│ P90 Response Time          │ 2.39 seconds                    │
│ P95 Response Time          │ 2.39 seconds                    │
│ P99 Response Time          │ 2.39 seconds                    │
│ Std Dev Response Time      │ 0.04 seconds                    │
│ Theoretical Max Throughput │ 41.91 requests/second           │
│ Actual Throughput          │ 4.37 requests/second            │
│ Total Generated Tokens     │ 10235                           │
│ Tokens Per Second          │ 447.39                          │
│ Avg Tokens Per Request     │ 102.35                          │
│ Peak Requests In Flight    │ 10                              │
└────────────────────────────┴─────────────────────────────────┘

Sample Response:
 The first line should be "In the silent hum of ci...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi sglang-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./sglang-stress-test.py
Starting SGLang Stress Test
URL: http://localhost:30000/generate
Max Tokens: 256
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 22.97 seconds
        SGLang Stress Test Results - 2025-05-23 20:34:59        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                           ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:30000/generate │
│ Max Tokens                 │ 256                             │
│ Concurrent Requests        │ 10                              │
│ Total Requests             │ 100                             │
│ Successful Requests        │ 100 (100.0%)                    │
│ Failed Requests            │ 0 (0.0%)                        │
│ Total Test Duration        │ 22.97 seconds                   │
│ Min Response Time          │ 2.28 seconds                    │
│ Max Response Time          │ 2.33 seconds                    │
│ Average Response Time      │ 2.30 seconds                    │
│ Median Response Time       │ 2.29 seconds                    │
│ P90 Response Time          │ 2.33 seconds                    │
│ P95 Response Time          │ 2.33 seconds                    │
│ P99 Response Time          │ 2.33 seconds                    │
│ Std Dev Response Time      │ 0.01 seconds                    │
│ Theoretical Max Throughput │ 42.88 requests/second           │
│ Actual Throughput          │ 4.35 requests/second            │
│ Total Generated Tokens     │ 10227                           │
│ Tokens Per Second          │ 445.28                          │
│ Avg Tokens Per Request     │ 102.27                          │
│ Peak Requests In Flight    │ 10                              │
└────────────────────────────┴─────────────────────────────────┘

Sample Response:
 A good API is a set of rules and protocols that a...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi sglang-stress-test.py
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./sglang-stress-test.py
Starting SGLang Stress Test
URL: http://localhost:30000/generate
Max Tokens: 256
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 22.88 seconds
        SGLang Stress Test Results - 2025-05-23 20:35:57        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                           ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ URL                        │ http://localhost:30000/generate │
│ Max Tokens                 │ 256                             │
│ Concurrent Requests        │ 10                              │
│ Total Requests             │ 100                             │
│ Successful Requests        │ 100 (100.0%)                    │
│ Failed Requests            │ 0 (0.0%)                        │
│ Total Test Duration        │ 22.88 seconds                   │
│ Min Response Time          │ 2.28 seconds                    │
│ Max Response Time          │ 2.30 seconds                    │
│ Average Response Time      │ 2.29 seconds                    │
│ Median Response Time       │ 2.29 seconds                    │
│ P90 Response Time          │ 2.30 seconds                    │
│ P95 Response Time          │ 2.30 seconds                    │
│ P99 Response Time          │ 2.30 seconds                    │
│ Std Dev Response Time      │ 0.01 seconds                    │
│ Theoretical Max Throughput │ 43.46 requests/second           │
│ Actual Throughput          │ 4.37 requests/second            │
│ Total Generated Tokens     │ 10225                           │
│ Tokens Per Second          │ 446.88                          │
│ Avg Tokens Per Request     │ 102.25                          │
│ Peak Requests In Flight    │ 10                              │
└────────────────────────────┴─────────────────────────────────┘

Sample Response:
 1. Fault Tolerance: The system should be able to ...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ls
prompt.txt  requirements.txt  sglang-stress-test.py  val3.py  vllm-stress-test.py  号百反诈测试集.xlsx
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ls
openai-stress-test.py  prompt.txt  requirements.txt  sglang-stress-test.py  val3.py  vllm-stress-test.py  号百反诈测试集.xlsx
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi openai-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./!$
./openai-stress-test.py
-bash: ./openai-stress-test.py: Permission denied
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# chmod +x openai-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./openai-stress-test.py
  File "/home/scam/./openai-stress-test.py", line 71
    timeout=DEFAULT_TIMEOUT
            ^^^^^^^^^^^^^^^
SyntaxError: invalid syntax. Perhaps you forgot a comma?
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./openai-stress-test.py
  File "/home/scam/./openai-stress-test.py", line 71
    timeout=DEFAULT_TIMEOUT
            ^^^^^^^^^^^^^^^
SyntaxError: invalid syntax. Perhaps you forgot a comma?
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi openai-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./openai-stress-test.py
Starting LLM Stress Test
API Base: http://localhost:30000/v1
Model: qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 40.71 seconds
      LLM Stress Test Results - 2025-05-23 20:39:55       
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                     ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ API Base                   │ http://localhost:30000/v1 │
│ Model                      │ qwen3-4B                  │
│ Max Tokens                 │ 256                       │
│ Temperature                │ 0.7                       │
│ Concurrent Requests        │ 10                        │
│ Total Requests             │ 100                       │
│ Successful Requests        │ 100 (100.0%)              │
│ Failed Requests            │ 0 (0.0%)                  │
│ Total Test Duration        │ 40.71 seconds             │
│ Min Response Time          │ 1.69 seconds              │
│ Max Response Time          │ 4.58 seconds              │
│ Average Response Time      │ 3.82 seconds              │
│ Median Response Time       │ 4.52 seconds              │
│ P90 Response Time          │ 4.58 seconds              │
│ P95 Response Time          │ 4.58 seconds              │
│ P99 Response Time          │ 4.58 seconds              │
│ Std Dev Response Time      │ 0.92 seconds              │
│ Theoretical Max Throughput │ 21.82 requests/second     │
│ Actual Throughput          │ 2.46 requests/second      │
│ Total Generated Tokens     │ 16327                     │
│ Tokens Per Second          │ 401.09                    │
│ Avg Tokens Per Request     │ 163.27                    │
│ Peak Requests In Flight    │ 10                        │
└────────────────────────────┴───────────────────────────┘

Sample Response:
Securing a web application is a critical aspect of...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi openai-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./openai-stress-test.py
Starting LLM Stress Test
API Base: http://localhost:30000/v1
Model: qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 38.13 seconds
      LLM Stress Test Results - 2025-05-23 20:41:03       
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                     ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ API Base                   │ http://localhost:30000/v1 │
│ Model                      │ qwen3-4B                  │
│ Max Tokens                 │ 256                       │
│ Temperature                │ 0.7                       │
│ Concurrent Requests        │ 10                        │
│ Total Requests             │ 100                       │
│ Successful Requests        │ 100 (100.0%)              │
│ Failed Requests            │ 0 (0.0%)                  │
│ Total Test Duration        │ 38.13 seconds             │
│ Min Response Time          │ 1.53 seconds              │
│ Max Response Time          │ 4.58 seconds              │
│ Average Response Time      │ 3.46 seconds              │
│ Median Response Time       │ 3.44 seconds              │
│ P90 Response Time          │ 4.57 seconds              │
│ P95 Response Time          │ 4.58 seconds              │
│ P99 Response Time          │ 4.58 seconds              │
│ Std Dev Response Time      │ 0.92 seconds              │
│ Theoretical Max Throughput │ 21.82 requests/second     │
│ Actual Throughput          │ 2.62 requests/second      │
│ Total Generated Tokens     │ 15496                     │
│ Tokens Per Second          │ 406.37                    │
│ Avg Tokens Per Request     │ 154.96                    │
│ Peak Requests In Flight    │ 10                        │
└────────────────────────────┴───────────────────────────┘

Sample Response:
Securing a web application is a critical part of d...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi openai-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./openai-stress-test.py
Starting LLM Stress Test
API Base: http://localhost:8000/v1
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 27.28 seconds
        LLM Stress Test Results - 2025-05-23 20:43:11        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ API Base                   │ http://localhost:8000/v1     │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B │
│ Max Tokens                 │ 256                          │
│ Temperature                │ 0.7                          │
│ Concurrent Requests        │ 10                           │
│ Total Requests             │ 100                          │
│ Successful Requests        │ 100 (100.0%)                 │
│ Failed Requests            │ 0 (0.0%)                     │
│ Total Test Duration        │ 27.28 seconds                │
│ Min Response Time          │ 1.06 seconds                 │
│ Max Response Time          │ 3.17 seconds                 │
│ Average Response Time      │ 2.38 seconds                 │
│ Median Response Time       │ 2.66 seconds                 │
│ P90 Response Time          │ 2.71 seconds                 │
│ P95 Response Time          │ 3.06 seconds                 │
│ P99 Response Time          │ 3.17 seconds                 │
│ Std Dev Response Time      │ 0.61 seconds                 │
│ Theoretical Max Throughput │ 31.52 requests/second        │
│ Actual Throughput          │ 3.67 requests/second         │
│ Total Generated Tokens     │ 15838                        │
│ Tokens Per Second          │ 580.65                       │
│ Avg Tokens Per Request     │ 158.38                       │
│ Peak Requests In Flight    │ 10                           │
└────────────────────────────┴──────────────────────────────┘

Sample Response:
The difference between **supervised** and **unsupe...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi openai-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./openai-stress-test.py
Starting LLM Stress Test
API Base: http://localhost:8000/v1
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 27.36 seconds
        LLM Stress Test Results - 2025-05-23 20:45:25        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ API Base                   │ http://localhost:8000/v1     │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B │
│ Max Tokens                 │ 256                          │
│ Temperature                │ 0.7                          │
│ Concurrent Requests        │ 10                           │
│ Total Requests             │ 100                          │
│ Successful Requests        │ 100 (100.0%)                 │
│ Failed Requests            │ 0 (0.0%)                     │
│ Total Test Duration        │ 27.36 seconds                │
│ Min Response Time          │ 2.66 seconds                 │
│ Max Response Time          │ 2.81 seconds                 │
│ Average Response Time      │ 2.71 seconds                 │
│ Median Response Time       │ 2.71 seconds                 │
│ P90 Response Time          │ 2.73 seconds                 │
│ P95 Response Time          │ 2.73 seconds                 │
│ P99 Response Time          │ 2.81 seconds                 │
│ Std Dev Response Time      │ 0.01 seconds                 │
│ Theoretical Max Throughput │ 35.61 requests/second        │
│ Actual Throughput          │ 3.65 requests/second         │
│ Total Generated Tokens     │ 20571                        │
│ Tokens Per Second          │ 751.75                       │
│ Avg Tokens Per Request     │ 205.71                       │
│ Peak Requests In Flight    │ 10                           │
└────────────────────────────┴──────────────────────────────┘

Sample Response:
<think>
Okay, the user is asking how virtualizatio...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi openai-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./openai-stress-test.py
Starting LLM Stress Test
API Base: http://localhost:8000/v1
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 27.34 seconds
        LLM Stress Test Results - 2025-05-23 20:46:19        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ API Base                   │ http://localhost:8000/v1     │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B │
│ Max Tokens                 │ 256                          │
│ Temperature                │ 0.7                          │
│ Concurrent Requests        │ 10                           │
│ Total Requests             │ 100                          │
│ Successful Requests        │ 100 (100.0%)                 │
│ Failed Requests            │ 0 (0.0%)                     │
│ Total Test Duration        │ 27.34 seconds                │
│ Min Response Time          │ 2.67 seconds                 │
│ Max Response Time          │ 2.81 seconds                 │
│ Average Response Time      │ 2.71 seconds                 │
│ Median Response Time       │ 2.71 seconds                 │
│ P90 Response Time          │ 2.72 seconds                 │
│ P95 Response Time          │ 2.72 seconds                 │
│ P99 Response Time          │ 2.81 seconds                 │
│ Std Dev Response Time      │ 0.01 seconds                 │
│ Theoretical Max Throughput │ 35.59 requests/second        │
│ Actual Throughput          │ 3.66 requests/second         │
│ Total Generated Tokens     │ 20630                        │
│ Tokens Per Second          │ 754.46                       │
│ Avg Tokens Per Request     │ 206.30                       │
│ Peak Requests In Flight    │ 10                           │
└────────────────────────────┴──────────────────────────────┘

Sample Response:
<think>
Okay, the user is asking me to explain wha...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi openai-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# vi openai-stress-test.py 
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./!$
./openai-stress-test.py
Starting LLM Stress Test
API Base: http://localhost:30000/v1
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 45.33 seconds
        LLM Stress Test Results - 2025-05-23 20:49:55        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ API Base                   │ http://localhost:30000/v1    │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B │
│ Max Tokens                 │ 256                          │
│ Temperature                │ 0.7                          │
│ Concurrent Requests        │ 10                           │
│ Total Requests             │ 100                          │
│ Successful Requests        │ 100 (100.0%)                 │
│ Failed Requests            │ 0 (0.0%)                     │
│ Total Test Duration        │ 45.33 seconds                │
│ Min Response Time          │ 4.46 seconds                 │
│ Max Response Time          │ 4.70 seconds                 │
│ Average Response Time      │ 4.51 seconds                 │
│ Median Response Time       │ 4.50 seconds                 │
│ P90 Response Time          │ 4.59 seconds                 │
│ P95 Response Time          │ 4.59 seconds                 │
│ P99 Response Time          │ 4.70 seconds                 │
│ Std Dev Response Time      │ 0.04 seconds                 │
│ Theoretical Max Throughput │ 21.26 requests/second        │
│ Actual Throughput          │ 2.21 requests/second         │
│ Total Generated Tokens     │ 20608                        │
│ Tokens Per Second          │ 454.63                       │
│ Avg Tokens Per Request     │ 206.08                       │
│ Peak Requests In Flight    │ 10                           │
└────────────────────────────┴──────────────────────────────┘

Sample Response:
<think>
Okay, the user is asking how virtualizatio...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# ./openai-stress-test.py
Starting LLM Stress Test
API Base: http://localhost:30000/v1
Model: /home/vllm/llm/Qwen/Qwen3-4B
Max Tokens: 256
Temperature: 0.7
Concurrent requests: 10
Total requests: 100
Processing requests... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00
Test completed in 45.36 seconds
        LLM Stress Test Results - 2025-05-23 20:50:57        
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Metric                     ┃ Value                        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ API Base                   │ http://localhost:30000/v1    │
│ Model                      │ /home/vllm/llm/Qwen/Qwen3-4B │
│ Max Tokens                 │ 256                          │
│ Temperature                │ 0.7                          │
│ Concurrent Requests        │ 10                           │
│ Total Requests             │ 100                          │
│ Successful Requests        │ 100 (100.0%)                 │
│ Failed Requests            │ 0 (0.0%)                     │
│ Total Test Duration        │ 45.36 seconds                │
│ Min Response Time          │ 4.46 seconds                 │
│ Max Response Time          │ 4.65 seconds                 │
│ Average Response Time      │ 4.51 seconds                 │
│ Median Response Time       │ 4.50 seconds                 │
│ P90 Response Time          │ 4.55 seconds                 │
│ P95 Response Time          │ 4.56 seconds                 │
│ P99 Response Time          │ 4.65 seconds                 │
│ Std Dev Response Time      │ 0.03 seconds                 │
│ Theoretical Max Throughput │ 21.51 requests/second        │
│ Actual Throughput          │ 2.20 requests/second         │
│ Total Generated Tokens     │ 20694                        │
│ Tokens Per Second          │ 456.18                       │
│ Avg Tokens Per Request     │ 206.94                       │
│ Peak Requests In Flight    │ 10                           │
└────────────────────────────┴──────────────────────────────┘

Sample Response:
<think>
Okay, the user is asking about the differe...
root@iZ2ze59h1tjhd32gca1wtxZ:/home/scam# 
```

